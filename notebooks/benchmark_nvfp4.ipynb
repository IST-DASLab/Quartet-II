{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0311d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=8\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb33cd",
   "metadata": {},
   "source": [
    "## Triton Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5697131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 64 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def rtn_1x16s_fp4_kernel(\n",
    "    x_ptr,\n",
    "    amax_ptr,\n",
    "    output_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    scale_override: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):        \n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # amax\n",
    "    scales_max = 447.99\n",
    "    val_max = 6.0 / scale_override\n",
    "    amax = tl.load(amax_ptr)\n",
    "    s_dec = tl.where(\n",
    "        amax == 0.0,\n",
    "        1.0,\n",
    "        amax / scales_max / val_max,\n",
    "    )\n",
    "    \n",
    "    # group\n",
    "    x_grouped = tl.reshape(x_flat, (BLOCK_SIZE // group_size, group_size))\n",
    "    \n",
    "    # scale\n",
    "    s_dec_b = tl.max(tl.abs(x_grouped), axis=-1, keep_dims=True) / val_max\n",
    "    s_dec_b_e4m3 = (s_dec_b / s_dec).to(tl.float8e4nv).to(tl.float32)\n",
    "    s_dec_b_e4m3 = tl.where(\n",
    "        s_dec_b_e4m3 == 0,\n",
    "        1.0,\n",
    "        s_dec_b_e4m3,\n",
    "    )\n",
    "    s_enc_b_inv = s_dec_b_e4m3 * s_dec\n",
    "    x_scaled = x_grouped / s_enc_b_inv\n",
    "    \n",
    "    # quantize\n",
    "    x_scaled_abs = tl.abs(x_scaled)\n",
    "    x_scaled_sign = tl.where(\n",
    "        x_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    x_fp4_abs = tl.where(\n",
    "        x_scaled_abs >= 5,\n",
    "        6,\n",
    "        tl.where(\n",
    "            x_scaled_abs >= 3.5,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_scaled_abs >= 2.5,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_scaled_abs >= 1.75,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_scaled_abs >= 1.25,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_scaled_abs >= 0.75,\n",
    "                            1,\n",
    "                            tl.where(\n",
    "                                x_scaled_abs >= 0.25,\n",
    "                                0.5,\n",
    "                                0.0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    x_fp4 = x_fp4_abs * x_scaled_sign\n",
    "\n",
    "    # dequantize\n",
    "    x_dequantized = x_fp4 * s_enc_b_inv\n",
    "    \n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "    \n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "@torch.compiler.disable()\n",
    "def rtn_1x16s_fp4_kernel_wrapper(\n",
    "    x: torch.Tensor,\n",
    "    scale_override: float,\n",
    "    group_size: int,\n",
    ") -> torch.Tensor:\n",
    "    x = x.contiguous()\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    \n",
    "    rtn_1x16s_fp4_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        amax_ptr=x.abs().max(),\n",
    "        output_ptr=output,\n",
    "        n_elements=n_elements,\n",
    "        scale_override=scale_override,\n",
    "        group_size=group_size,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d922ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 64 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def eden_1x16s_fp4_kernel(\n",
    "    x_ptr,\n",
    "    hadamard_matrix_ptr,\n",
    "    current_amax_ptr,\n",
    "    output_ptr,\n",
    "    next_amax_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    hadamard_dim: tl.constexpr,\n",
    "    scale_override: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    seed: int,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):    \n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # hadamard transform\n",
    "    offsets_hadamard = tl.arange(0, hadamard_dim * hadamard_dim)\n",
    "    hadamard_matrix = tl.load(hadamard_matrix_ptr + offsets_hadamard).reshape(hadamard_dim, hadamard_dim) \n",
    "    x = tl.reshape(x_flat, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_had = tl.dot(x, hadamard_matrix) # not TN!, A @ B!\n",
    "    \n",
    "    # write amax for next iter\n",
    "    tl.atomic_max(next_amax_ptr, tl.max(tl.abs(x_had)), sem=\"relaxed\")\n",
    "    \n",
    "    # group\n",
    "    x_grouped = tl.reshape(x_had, (BLOCK_SIZE // group_size, group_size))\n",
    "\n",
    "    # amax\n",
    "    scales_max = 255.99 # Not 448 because eden needs space to rescale up a bit sometimes after the correction\n",
    "    val_max = 6.0 / scale_override\n",
    "    amax = tl.load(current_amax_ptr)\n",
    "    s_dec = tl.where(\n",
    "        amax == 0.0,\n",
    "        1.0,\n",
    "        amax / scales_max / val_max,\n",
    "    )\n",
    "    \n",
    "    # scale\n",
    "    s_dec_b = tl.max(tl.abs(x_grouped), axis=-1, keep_dims=True) / val_max\n",
    "    s_dec_b_e4m3 = (s_dec_b / s_dec).to(tl.float8e4nv).to(tl.float32)\n",
    "    s_dec_b_e4m3 = tl.where(\n",
    "        s_dec_b_e4m3 == 0,\n",
    "        1.0,\n",
    "        s_dec_b_e4m3,\n",
    "    )\n",
    "    x_scaled = x_grouped / (s_dec_b_e4m3 * s_dec)\n",
    "    \n",
    "    # quantize\n",
    "    x_scaled_abs = tl.abs(x_scaled)\n",
    "    x_scaled_sign = tl.where(\n",
    "        x_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    x_fp4 = tl.where(\n",
    "        x_scaled_abs >= 5,\n",
    "        6,\n",
    "        tl.where(\n",
    "            x_scaled_abs >= 3.5,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_scaled_abs >= 2.5,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_scaled_abs >= 1.75,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_scaled_abs >= 1.25,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_scaled_abs >= 0.75,\n",
    "                            1,\n",
    "                            tl.where(\n",
    "                                x_scaled_abs >= 0.25,\n",
    "                                0.5,\n",
    "                                0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) * x_scaled_sign\n",
    "    \n",
    "    # Calculate EDEN scale\n",
    "    x_scaled = tl.reshape(x_scaled, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_fp4 = tl.reshape(x_fp4, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    \n",
    "    num = tl.sum(x_scaled * x_scaled, axis=-1, keep_dims=True)\n",
    "    denom = tl.sum(x_scaled * x_fp4, axis=-1, keep_dims=True)\n",
    "    \n",
    "    correction = tl.where(\n",
    "        denom == 0.0,\n",
    "        1.0,\n",
    "        num / denom,\n",
    "    )\n",
    "    \n",
    "    # Apply EDEN scale\n",
    "    scales = tl.reshape(s_dec_b_e4m3, (BLOCK_SIZE // hadamard_dim, hadamard_dim // group_size))\n",
    "    corrected_scales = tl.reshape(scales * correction, (BLOCK_SIZE // group_size, 1))\n",
    "    \n",
    "    bitscales = tl.cast(corrected_scales.to(tl.float8e4nv), tl.uint8, bitcast=True)\n",
    "    prevscale = tl.cast((bitscales - 1), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    currscale = tl.cast((bitscales), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    nextscale = tl.cast((bitscales + 1), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    \n",
    "    up = tl.where(\n",
    "        currscale > corrected_scales,\n",
    "        currscale,\n",
    "        nextscale,\n",
    "    )\n",
    "    down = tl.where(\n",
    "        currscale > corrected_scales,\n",
    "        prevscale,\n",
    "        currscale,\n",
    "    )\n",
    "    \n",
    "    prob_up = (corrected_scales - down) / (up - down)\n",
    "    \n",
    "    scale_start_idx = pid * (BLOCK_SIZE // group_size)\n",
    "    scale_offsets = scale_start_idx + tl.arange(0, BLOCK_SIZE // group_size)\n",
    "    sampled_prob = tl.rand(seed, scale_offsets).reshape(BLOCK_SIZE // group_size, 1)\n",
    "    \n",
    "    scales = tl.where(\n",
    "        sampled_prob < prob_up,\n",
    "        up,\n",
    "        down,\n",
    "    )\n",
    "    scales = tl.reshape(scales, (BLOCK_SIZE // group_size, 1))\n",
    "    x_fp4 = tl.reshape(x_fp4, (BLOCK_SIZE // group_size, group_size))\n",
    "    \n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized = x_fp4 * scales * s_dec\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "    \n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "@torch.compiler.disable()\n",
    "def eden_1x16s_fp4_kernel_wrapper(\n",
    "    x: torch.Tensor,\n",
    "    hadamard_matrix: torch.Tensor,\n",
    "    scale_override: float,\n",
    "    group_size: int,\n",
    "    current_amax: torch.Tensor,\n",
    ") -> [torch.Tensor, torch.Tensor]:\n",
    "    hadamard_dim = hadamard_matrix.size(0)\n",
    "    assert hadamard_matrix.size(1) == hadamard_dim\n",
    "    assert x.numel() % hadamard_dim == 0\n",
    "    assert hadamard_dim % group_size == 0\n",
    "    \n",
    "    x = x.contiguous()\n",
    "    hadamard_matrix = hadamard_matrix.T.contiguous() # .T.contiguous() + tl.dot -> TN\n",
    "    output = torch.empty_like(x)\n",
    "    seed = randint(0, 1000000)\n",
    "    \n",
    "    next_amax = torch.zeros_like(current_amax)\n",
    "    \n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    \n",
    "    eden_1x16s_fp4_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        hadamard_matrix_ptr=hadamard_matrix,\n",
    "        current_amax_ptr=current_amax,\n",
    "        output_ptr=output,\n",
    "        next_amax_ptr=next_amax,\n",
    "        n_elements=n_elements,\n",
    "        hadamard_dim=hadamard_dim,\n",
    "        scale_override=scale_override,\n",
    "        group_size=group_size,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return output, next_amax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5731d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import hadamard\n",
    "\n",
    "def get_hadamard_matrix(group_size: int, dtype: torch.dtype, device: torch.device):\n",
    "    return torch.tensor(\n",
    "        hadamard(group_size) * group_size**-0.5,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "    \n",
    "def rerotate_hadamard(hadamard_matrix):\n",
    "    signs = torch.diag(\n",
    "        torch.randint(\n",
    "            0, 2, (hadamard_matrix.size(0),),\n",
    "            device=hadamard_matrix.device,\n",
    "            dtype=hadamard_matrix.dtype\n",
    "        ) * 2 - 1\n",
    "    )\n",
    "    return hadamard_matrix @ signs # NOTE: rerotate along last dim, inner dim for TN GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4015835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f7b0df91f04e178828d0cb070f4a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating steps:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac3c7c04eb54e64ab4344a6defd3f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 2.84 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b006c1f2974428faf7564c6c03cb051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 3.83 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b06d25422049f596dbe13638805516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 4.84 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c639b7bfbcf34181bc31093bd7ddd439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: 5.83 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7c293bdbf24d04ba9fc28b3f3bca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: 6.82 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80519cc16fb04074a49fc423a5bc3116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024: 7.74 bits\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "M = 1024\n",
    "N = 1024\n",
    "K = 1024\n",
    "HADAMARD_DIM = 32\n",
    "\n",
    "A = torch.randn((M, K), device='cuda')\n",
    "B = torch.randn((N, K), device='cuda')\n",
    "ht = get_hadamard_matrix(32, A.dtype, A.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for acc_steps in tqdm([1, 4, 16, 64, 256, 1024], desc=\"Iterating steps\"):\n",
    "        accumulator = torch.zeros_like(A @ B.T)\n",
    "        for i in trange(acc_steps, leave=False):\n",
    "            ht = rerotate_hadamard(ht)\n",
    "\n",
    "            A_amax_buffer = (A.view(-1, ht.size(0)) @ ht.T).abs().max()\n",
    "            Aq, A_amax_buffer = eden_1x16s_fp4_kernel_wrapper(\n",
    "                A,\n",
    "                ht,\n",
    "                1.0,\n",
    "                16,\n",
    "                current_amax=A_amax_buffer,\n",
    "            )\n",
    "            \n",
    "            B_amax_buffer = (A.view(-1, ht.size(0)) @ ht.T).abs().max()\n",
    "            Bq, B_amax_buffer = eden_1x16s_fp4_kernel_wrapper(\n",
    "                B,\n",
    "                ht,\n",
    "                1.0,\n",
    "                16,\n",
    "                current_amax=B_amax_buffer,\n",
    "            )\n",
    "            \n",
    "            accumulator += Aq @ Bq.T\n",
    "        accumulator /= acc_steps\n",
    "        \n",
    "        quad_err = (accumulator - A @ B.T).pow(2).mean() / (A @ B.T).pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        print(f\"{acc_steps}: {eff_bitwidth:.2f} bits\")\n",
    "        \n",
    "# NEED TO GROW BY ~1 bit per 4x samples\n",
    "# v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8013658",
   "metadata": {},
   "source": [
    "## Quartet_II linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmaxStorage:\n",
    "    def __init__(self):\n",
    "        self.e_ht_amax = None\n",
    "        self.weght_tht_amax = None\n",
    "        self.e_tht_amax = None\n",
    "        self.input_tht_amax = None\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        fields = [\n",
    "            (\"e_ht_amax\", self.e_ht_amax), \n",
    "            (\"weght_tht_amax\", self.weght_tht_amax), \n",
    "            (\"e_tht_amax\", self.e_tht_amax), \n",
    "            (\"input_tht_amax\", self.input_tht_amax)\n",
    "        ]\n",
    "        field_strs = []\n",
    "        for name, val in fields:\n",
    "            if val is not None:\n",
    "                try:\n",
    "                    v = val.item()\n",
    "                except Exception:\n",
    "                    v = val\n",
    "                field_strs.append(f\"{name}: {v:.3e}\")\n",
    "            else:\n",
    "                field_strs.append(f\"{name}: None\")\n",
    "        return \"<AmaxStorage \" + \", \".join(field_strs) + \">\"\n",
    "        \n",
    "\n",
    "class Quartet_II_fn(torch.autograd.Function):\n",
    "    group_size = 16\n",
    "    forward_scale_override = 1.0\n",
    "    backward_scale_override = (17 / 16) * 0.93\n",
    "    hadamard_matrix = get_hadamard_matrix(32, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    @torch.compile(dynamic=False)\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, amax_storage: AmaxStorage, delayed_amax: bool, disable_backward_quant: bool):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "        ctx.delayed_amax = delayed_amax\n",
    "        ctx.amax_storage = amax_storage\n",
    "        ctx.disable_backward_quant = disable_backward_quant\n",
    "        \n",
    "        input_fp4 = rtn_1x16s_fp4_kernel_wrapper(input, scale_override=Quartet_II_fn.forward_scale_override, group_size=Quartet_II_fn.group_size)\n",
    "        weight_fp4 = rtn_1x16s_fp4_kernel_wrapper(weight, scale_override=Quartet_II_fn.forward_scale_override, group_size=Quartet_II_fn.group_size)\n",
    "\n",
    "        ctx.save_for_backward(input_fp4, weight_fp4)\n",
    "        return F.linear(input_fp4, weight_fp4)\n",
    "\n",
    "    @torch.compile(dynamic=False)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Load ctx and reshape\n",
    "        input_fp4, weight_fp4 = ctx.saved_tensors\n",
    "        \n",
    "        input_fp4 = input_fp4.reshape(ctx.batch * ctx.seq, ctx.in_dim)\n",
    "        grad_output = grad_output.reshape(ctx.batch * ctx.seq, ctx.out_dim)\n",
    "        \n",
    "        # Re-randomize the rotation\n",
    "        Quartet_II_fn.hadamard_matrix = rerotate_hadamard(Quartet_II_fn.hadamard_matrix)\n",
    "        \n",
    "        # No backward quant if flag\n",
    "        if ctx.disable_backward_quant:\n",
    "            grad_input = F.linear(\n",
    "                grad_output,\n",
    "                weight_fp4.T,\n",
    "                None,\n",
    "            ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "            \n",
    "            grad_weight = F.linear(\n",
    "                grad_output.T,\n",
    "                input_fp4.T,\n",
    "                None,\n",
    "            )\n",
    "            return grad_input, grad_weight, None, None, None\n",
    "        \n",
    "        # EW\n",
    "        if ctx.amax_storage.e_ht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.e_ht_amax = (grad_output.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max()\n",
    "        e_ht_fp4, ctx.amax_storage.e_ht_amax = eden_1x16s_fp4_kernel_wrapper(grad_output, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, 16, ctx.amax_storage.e_ht_amax)\n",
    "        \n",
    "        if ctx.amax_storage.weght_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.weght_tht_amax = (weight_fp4.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max()\n",
    "        weight_tht_fp4, ctx.amax_storage.weght_tht_amax = eden_1x16s_fp4_kernel_wrapper(weight_fp4.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, 16, ctx.amax_storage.weght_tht_amax)\n",
    "        \n",
    "        grad_input = F.linear(\n",
    "            e_ht_fp4,\n",
    "            weight_tht_fp4,\n",
    "            None,\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        # EtX\n",
    "        if ctx.amax_storage.e_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.e_tht_amax = (grad_output.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max()\n",
    "        e_tht_fp4, ctx.amax_storage.e_tht_amax = eden_1x16s_fp4_kernel_wrapper(grad_output.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.e_tht_amax)\n",
    "        \n",
    "        if ctx.amax_storage.input_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.input_tht_amax = (input_fp4.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max()\n",
    "        input_tht_fp4, ctx.amax_storage.input_tht_amax = eden_1x16s_fp4_kernel_wrapper(input_fp4.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.input_tht_amax)\n",
    "        \n",
    "        grad_weight = F.linear(\n",
    "            e_tht_fp4,\n",
    "            input_tht_fp4,\n",
    "            None,\n",
    "        )\n",
    "        \n",
    "        return grad_input, grad_weight, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb932bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quartet_II_linear(torch.nn.Linear):\n",
    "    def __init__(self, *args, delayed_amax=False, disable_backward_quant=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.delayed_amax = delayed_amax\n",
    "        self.disable_backward_quant = disable_backward_quant\n",
    "        self.amax_storage = AmaxStorage()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, disable_backward_quant=False):\n",
    "        return Quartet_II_fn.apply(x, self.weight, self.amax_storage, self.delayed_amax, self.disable_backward_quant)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7ab6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 4\n",
    "SEQ = 16\n",
    "HID = 256\n",
    "DELAYED_AMAX = True\n",
    "\n",
    "INPUT = torch.randn((BATCH, SEQ, HID), device='cuda')\n",
    "TARGET = torch.randn((BATCH, SEQ, 1), device='cuda')\n",
    "\n",
    "W1 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "W2 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "W3 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "\n",
    "with torch.no_grad():\n",
    "    W1.weight /= (HID**0.5 * W1.weight.std())\n",
    "    W2.weight /= (HID**0.5 * W2.weight.std())\n",
    "    W3.weight /= (HID**0.5 * W3.weight.std())\n",
    "\n",
    "head = torch.randn(HID, 1, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a4616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1.weight.grad = None\n",
    "W2.weight.grad = None\n",
    "W3.weight.grad = None\n",
    "\n",
    "W1.disable_backward_quant = True\n",
    "W2.disable_backward_quant = True\n",
    "W3.disable_backward_quant = True\n",
    "\n",
    "hid = W1(INPUT)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W2(hid)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W3(hid)\n",
    "loss = (hid @ head - TARGET).pow(2).sum()\n",
    "loss.backward()\n",
    "\n",
    "w1_ref_grad = W1.weight.grad.clone().detach()\n",
    "w2_ref_grad = W2.weight.grad.clone().detach()\n",
    "w3_ref_grad = W3.weight.grad.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bac792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b4a49575a249cc823e1ac05e30e456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde207885804880804921afd2f0a3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 acc_steps:\n",
      "\tW1 grad err: 1.97 bits, 0.996 cosine\n",
      "\tW2 grad err: 2.71 bits, 0.983 cosine\n",
      "\tW3 grad err: 4.47 bits, 0.996 cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/micromamba/envs/llmb/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1391: UserWarning: Dynamo does not know how to trace the builtin `<unknown module>.Tensor.reshape.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\n",
      "If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\n",
      "If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n",
      "  torch._dynamo.utils.warn_once(explanation + \"\\n\" + \"\\n\".join(hints))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7e3b7f8b354b6eb6ccc935e539aa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 acc_steps:\n",
      "\tW1 grad err: 2.95 bits, 0.998 cosine\n",
      "\tW2 grad err: 3.74 bits, 1.000 cosine\n",
      "\tW3 grad err: 5.37 bits, 0.999 cosine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8feff8da674caab799d83ae7999406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 acc_steps:\n",
      "\tW1 grad err: 4.04 bits, 0.998 cosine\n",
      "\tW2 grad err: 4.73 bits, 0.997 cosine\n",
      "\tW3 grad err: 6.17 bits, 0.993 cosine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d146a994e13437a8a83c0e26721ddd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 acc_steps:\n",
      "\tW1 grad err: 5.01 bits, 0.995 cosine\n",
      "\tW2 grad err: 5.80 bits, 0.996 cosine\n",
      "\tW3 grad err: 7.15 bits, 0.997 cosine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5eeec437948e7bb52a3eddfcc5624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 acc_steps:\n",
      "\tW1 grad err: 5.93 bits, 0.995 cosine\n",
      "\tW2 grad err: 6.55 bits, 0.997 cosine\n",
      "\tW3 grad err: 8.08 bits, 1.000 cosine\n"
     ]
    }
   ],
   "source": [
    "W1.disable_backward_quant = False\n",
    "W2.disable_backward_quant = False\n",
    "W3.disable_backward_quant = False\n",
    "\n",
    "hid = W1(INPUT)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W2(hid)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W3(hid)\n",
    "loss = (hid @ head - TARGET).pow(2).sum()\n",
    "\n",
    "for acc_steps in tqdm([1, 4, 16, 64, 256]):\n",
    "    W1.weight.grad = None\n",
    "    W2.weight.grad = None\n",
    "    W3.weight.grad = None\n",
    "    for _ in trange(acc_steps, leave=False):\n",
    "        loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        W1.weight.grad /= acc_steps\n",
    "        W2.weight.grad /= acc_steps\n",
    "        W3.weight.grad /= acc_steps\n",
    "    \n",
    "        quad_err = (W1.weight.grad - w1_ref_grad).pow(2).mean() / w1_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W1.weight.grad.flatten() @ w1_ref_grad.flatten()) / (w1_ref_grad.flatten() @ w1_ref_grad.flatten())\n",
    "        print(f\"{acc_steps} acc_steps:\\n\\tW1 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "        \n",
    "        quad_err = (W2.weight.grad - w2_ref_grad).pow(2).mean() / w2_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W2.weight.grad.flatten() @ w2_ref_grad.flatten()) / (w2_ref_grad.flatten() @ w2_ref_grad.flatten())\n",
    "        print(f\"\\tW2 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "        \n",
    "        quad_err = (W3.weight.grad - w3_ref_grad).pow(2).mean() / w3_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W3.weight.grad.flatten() @ w3_ref_grad.flatten()) / (w3_ref_grad.flatten() @ w3_ref_grad.flatten())\n",
    "        print(f\"\\tW3 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "\n",
    "\n",
    "# NEED TO GROW BY ~1 bit per 4x samples\n",
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd77c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

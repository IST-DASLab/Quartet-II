{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0311d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=8\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=8\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb33cd",
   "metadata": {},
   "source": [
    "## Triton Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5697131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rtn_fp4(x):\n",
    "    x_abs = tl.abs(x)\n",
    "    x_sign = tl.where(\n",
    "        x > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    x_fp4_abs = tl.where(\n",
    "        x_abs >= 5,\n",
    "        6,\n",
    "        tl.where(\n",
    "            x_abs >= 3.5,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_abs >= 2.5,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_abs >= 1.75,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_abs >= 1.25,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_abs >= 0.75,\n",
    "                            1,\n",
    "                            tl.where(\n",
    "                                x_abs >= 0.25,\n",
    "                                0.5,\n",
    "                                0.0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return x_fp4_abs * x_sign\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def get_scales(x, amax, val_max, scales_max):\n",
    "    s_dec = tl.where(\n",
    "        amax == 0.0,\n",
    "        1.0,\n",
    "        amax / scales_max / val_max,\n",
    "    )\n",
    "    \n",
    "    s_dec_b = tl.max(tl.abs(x), axis=-1, keep_dims=True) / val_max\n",
    "    s_dec_b_e4m3 = (s_dec_b / s_dec).to(tl.float8e4nv).to(tl.float32)\n",
    "    s_dec_b_e4m3 = tl.where(\n",
    "        s_dec_b_e4m3 == 0,\n",
    "        1.0,\n",
    "        s_dec_b_e4m3,\n",
    "    )\n",
    "    return s_dec_b_e4m3, s_dec\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def get_alt_scales(x, val_max, s_dec):    \n",
    "    s_dec_b = tl.max(tl.abs(x), axis=-1, keep_dims=True) / val_max\n",
    "    s_dec_b_e4m3 = (s_dec_b * (6/4) / s_dec).to(tl.float8e4nv).to(tl.float32)\n",
    "    s_dec_b_e4m3 = tl.where(\n",
    "        s_dec_b_e4m3 == 0,\n",
    "        1.0,\n",
    "        s_dec_b_e4m3,\n",
    "    )\n",
    "    return s_dec_b_e4m3\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def rtn_1x16s_fp4_kernel(\n",
    "    x_ptr,\n",
    "    amax_ptr,\n",
    "    output_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    scale_override: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    four_over_six: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):        \n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # group\n",
    "    x_grouped = tl.reshape(x_flat, (BLOCK_SIZE // group_size, group_size))\n",
    "    \n",
    "    # amax\n",
    "    scales_max = 256.00 if four_over_six else 448.00\n",
    "    val_max = 6.0 / scale_override\n",
    "    amax = tl.load(amax_ptr)\n",
    "    \n",
    "    s_dec_b_e4m3, s_dec = get_scales(x_grouped, amax, val_max, scales_max)\n",
    "    x_scaled = x_grouped / (s_dec_b_e4m3 * s_dec)\n",
    "    \n",
    "    # quantize\n",
    "    x_fp4 = rtn_fp4(x_scaled)\n",
    "    x_dequantized = x_fp4 * (s_dec_b_e4m3 * s_dec)\n",
    "    \n",
    "    if not four_over_six:\n",
    "        best_x_dequantized = x_dequantized\n",
    "    else:\n",
    "        alt_s_dec_b_e4m3 = get_alt_scales(x_grouped, val_max, s_dec)\n",
    "        alt_x_scaled = x_grouped / (alt_s_dec_b_e4m3 * s_dec)\n",
    "        \n",
    "        alt_x_fp4 = rtn_fp4(alt_x_scaled)\n",
    "        alt_x_dequantized = alt_x_fp4 * (alt_s_dec_b_e4m3 * s_dec)\n",
    "        \n",
    "        error_six = tl.sum((x_grouped - x_dequantized) * (x_grouped - x_dequantized), axis=-1, keep_dims=True)\n",
    "        error_four = tl.sum((x_grouped - alt_x_dequantized) * (x_grouped - alt_x_dequantized), axis=-1, keep_dims=True)\n",
    "        \n",
    "        best_x_dequantized = tl.where(\n",
    "            error_six <= error_four,\n",
    "            x_dequantized,\n",
    "            alt_x_dequantized,\n",
    "        )\n",
    "    \n",
    "    x_dequantized_flat = tl.reshape(best_x_dequantized, (BLOCK_SIZE,))\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "\n",
    "@torch.compiler.disable()\n",
    "def rtn_1x16s_fp4_kernel_wrapper(\n",
    "    x,\n",
    "    scale_override: float,\n",
    "    group_size: int,\n",
    "    four_over_six: bool,\n",
    "):\n",
    "    x = x.contiguous()\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    \n",
    "    rtn_1x16s_fp4_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        amax_ptr=x.abs().max(),\n",
    "        output_ptr=output,\n",
    "        n_elements=n_elements,\n",
    "        scale_override=scale_override,\n",
    "        group_size=group_size,\n",
    "        four_over_six=four_over_six,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d922ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 128 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 256 * 32}),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512 * 32}),\n",
    "    ],\n",
    "    key=[],\n",
    ")\n",
    "@triton.jit\n",
    "def eden_1x16s_fp4_kernel(\n",
    "    x_ptr,\n",
    "    hadamard_matrix_ptr,\n",
    "    current_amax_ptr,\n",
    "    output_ptr,\n",
    "    next_amax_ptr,\n",
    "    n_elements: tl.constexpr,\n",
    "    hadamard_dim: tl.constexpr,\n",
    "    scale_override: tl.constexpr,\n",
    "    group_size: tl.constexpr,\n",
    "    seed: int,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):    \n",
    "    # load x\n",
    "    pid = tl.program_id(0)\n",
    "    start_idx = pid * BLOCK_SIZE\n",
    "    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x_flat = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # hadamard transform\n",
    "    offsets_hadamard = tl.arange(0, hadamard_dim * hadamard_dim)\n",
    "    hadamard_matrix = tl.load(hadamard_matrix_ptr + offsets_hadamard).reshape(hadamard_dim, hadamard_dim) \n",
    "    x = tl.reshape(x_flat, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_had = tl.dot(x, hadamard_matrix) # not TN!, A @ B!\n",
    "    \n",
    "    # write amax for next iter\n",
    "    tl.atomic_max(next_amax_ptr, tl.max(tl.abs(x_had)).to(tl.float32), sem=\"relaxed\")\n",
    "    \n",
    "    # group\n",
    "    x_grouped = tl.reshape(x_had, (BLOCK_SIZE // group_size, group_size))\n",
    "\n",
    "    # amax\n",
    "    scales_max = 255.99 # Not 448 because eden needs space to rescale up a bit sometimes after the correction\n",
    "    val_max = 6.0 / scale_override\n",
    "    amax = tl.load(current_amax_ptr)\n",
    "    s_dec = tl.where(\n",
    "        amax == 0.0,\n",
    "        1.0,\n",
    "        amax / scales_max / val_max,\n",
    "    )\n",
    "    \n",
    "    # scale\n",
    "    s_dec_b = tl.max(tl.abs(x_grouped), axis=-1, keep_dims=True) / val_max\n",
    "    s_dec_b_e4m3 = (s_dec_b / s_dec).to(tl.float8e4nv).to(tl.float32)\n",
    "    s_dec_b_e4m3 = tl.where(\n",
    "        s_dec_b_e4m3 == 0,\n",
    "        1.0,\n",
    "        s_dec_b_e4m3,\n",
    "    )\n",
    "    x_scaled = x_grouped / (s_dec_b_e4m3 * s_dec)\n",
    "    \n",
    "    # quantize\n",
    "    x_scaled_abs = tl.abs(x_scaled)\n",
    "    x_scaled_sign = tl.where(\n",
    "        x_scaled > 0,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "    x_fp4 = tl.where(\n",
    "        x_scaled_abs >= 5,\n",
    "        6,\n",
    "        tl.where(\n",
    "            x_scaled_abs >= 3.5,\n",
    "            4,\n",
    "            tl.where(\n",
    "                x_scaled_abs >= 2.5,\n",
    "                3,\n",
    "                tl.where(\n",
    "                    x_scaled_abs >= 1.75,\n",
    "                    2,\n",
    "                    tl.where(\n",
    "                        x_scaled_abs >= 1.25,\n",
    "                        1.5,\n",
    "                        tl.where(\n",
    "                            x_scaled_abs >= 0.75,\n",
    "                            1,\n",
    "                            tl.where(\n",
    "                                x_scaled_abs >= 0.25,\n",
    "                                0.5,\n",
    "                                0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) * x_scaled_sign\n",
    "    \n",
    "    # Calculate EDEN scale\n",
    "    x_scaled = tl.reshape(x_scaled, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    x_fp4 = tl.reshape(x_fp4, (BLOCK_SIZE // hadamard_dim, hadamard_dim))\n",
    "    \n",
    "    num = tl.sum(x_scaled * x_scaled, axis=-1, keep_dims=True)\n",
    "    denom = tl.sum(x_scaled * x_fp4, axis=-1, keep_dims=True)\n",
    "    \n",
    "    correction = tl.where(\n",
    "        denom == 0.0,\n",
    "        1.0,\n",
    "        num / denom,\n",
    "    )\n",
    "    \n",
    "    # Apply EDEN scale\n",
    "    scales = tl.reshape(s_dec_b_e4m3, (BLOCK_SIZE // hadamard_dim, hadamard_dim // group_size))\n",
    "    corrected_scales = tl.reshape(scales * correction, (BLOCK_SIZE // group_size, 1))\n",
    "    \n",
    "    bitscales = tl.cast(corrected_scales.to(tl.float8e4nv), tl.uint8, bitcast=True)\n",
    "    prevscale = tl.cast((bitscales - 1), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    currscale = tl.cast((bitscales), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    nextscale = tl.cast((bitscales + 1), tl.float8e4nv, bitcast=True).to(tl.float32)\n",
    "    \n",
    "    up = tl.where(\n",
    "        currscale > corrected_scales,\n",
    "        currscale,\n",
    "        nextscale,\n",
    "    )\n",
    "    down = tl.where(\n",
    "        currscale > corrected_scales,\n",
    "        prevscale,\n",
    "        currscale,\n",
    "    )\n",
    "    \n",
    "    prob_up = (corrected_scales - down) / (up - down)\n",
    "    \n",
    "    scale_start_idx = pid * (BLOCK_SIZE // group_size)\n",
    "    scale_offsets = scale_start_idx + tl.arange(0, BLOCK_SIZE // group_size)\n",
    "    sampled_prob = tl.rand(seed, scale_offsets).reshape(BLOCK_SIZE // group_size, 1)\n",
    "    \n",
    "    scales = tl.where(\n",
    "        sampled_prob < prob_up,\n",
    "        up,\n",
    "        down,\n",
    "    )\n",
    "    scales = tl.reshape(scales, (BLOCK_SIZE // group_size, 1))\n",
    "    x_fp4 = tl.reshape(x_fp4, (BLOCK_SIZE // group_size, group_size))\n",
    "    \n",
    "    # Reshape back to flat form for storage\n",
    "    x_dequantized = x_fp4 * scales * s_dec\n",
    "    x_dequantized_flat = tl.reshape(x_dequantized, (BLOCK_SIZE,))\n",
    "    \n",
    "    # store\n",
    "    tl.store(output_ptr + offsets, x_dequantized_flat, mask=mask)\n",
    "\n",
    "@torch.compiler.disable()\n",
    "def eden_1x16s_fp4_kernel_wrapper(\n",
    "    x: torch.Tensor,\n",
    "    hadamard_matrix: torch.Tensor,\n",
    "    scale_override: float,\n",
    "    group_size: int,\n",
    "    current_amax: torch.Tensor,\n",
    ") -> [torch.Tensor, torch.Tensor]:\n",
    "    hadamard_dim = hadamard_matrix.size(0)\n",
    "    assert hadamard_matrix.size(1) == hadamard_dim\n",
    "    assert (x.numel() // x.shape[-1]) % hadamard_dim == 0\n",
    "    assert hadamard_dim % group_size == 0\n",
    "    \n",
    "    x = x.contiguous()\n",
    "    hadamard_matrix = hadamard_matrix.T.contiguous() # .T.contiguous() + tl.dot -> TN\n",
    "    output = torch.empty_like(x)\n",
    "    seed = randint(0, 1000000)\n",
    "    \n",
    "    next_amax = torch.zeros_like(current_amax)\n",
    "    \n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    \n",
    "    eden_1x16s_fp4_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        hadamard_matrix_ptr=hadamard_matrix,\n",
    "        current_amax_ptr=current_amax,\n",
    "        output_ptr=output,\n",
    "        next_amax_ptr=next_amax,\n",
    "        n_elements=n_elements,\n",
    "        hadamard_dim=hadamard_dim,\n",
    "        scale_override=scale_override,\n",
    "        group_size=group_size,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return output, next_amax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5731d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import hadamard\n",
    "\n",
    "def get_hadamard_matrix(group_size: int, dtype: torch.dtype, device: torch.device):\n",
    "    return torch.tensor(\n",
    "        hadamard(group_size) * group_size**-0.5,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "    \n",
    "def rerotate_hadamard(hadamard_matrix):\n",
    "    signs = torch.diag(\n",
    "        torch.randint(\n",
    "            0, 2, (hadamard_matrix.size(0),),\n",
    "            device=hadamard_matrix.device,\n",
    "            dtype=hadamard_matrix.dtype\n",
    "        ) * 2 - 1\n",
    "    )\n",
    "    return hadamard_matrix @ signs # NOTE: rerotate along last dim, inner dim for TN GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4015835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/micromamba/envs/llmb/lib/python3.10/site-packages/torch/cuda/__init__.py:799: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacacc2523f047449572e69cf8f511af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating steps:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61696080cbbf4093a91ea6abfea0a9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 2.84 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bddab4c4c342d3802bac314367d7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 3.84 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b2bab57585456ebbfad65d9c0b07d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 4.84 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78752c83301468ab376c017c9305693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: 5.83 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212876d216ae487e902d5562c38509f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: 6.81 bits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1368cef6344bf989fbc476c1de3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024: 7.75 bits\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "M = 1024\n",
    "N = 1024\n",
    "K = 1024\n",
    "HADAMARD_DIM = 128\n",
    "\n",
    "A = torch.randn((M, K), device='cuda')\n",
    "B = torch.randn((N, K), device='cuda')\n",
    "ht = get_hadamard_matrix(32, A.dtype, A.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for acc_steps in tqdm([1, 4, 16, 64, 256, 1024], desc=\"Iterating steps\"):\n",
    "        accumulator = torch.zeros_like(A @ B.T)\n",
    "        for i in trange(acc_steps, leave=False):\n",
    "            ht = rerotate_hadamard(ht)\n",
    "\n",
    "            A_amax_buffer = (A.view(-1, ht.size(0)) @ ht.T).abs().max()\n",
    "            Aq, A_amax_buffer = eden_1x16s_fp4_kernel_wrapper(\n",
    "                A,\n",
    "                ht,\n",
    "                1.0,\n",
    "                16,\n",
    "                current_amax=A_amax_buffer,\n",
    "            )\n",
    "            \n",
    "            B_amax_buffer = (A.view(-1, ht.size(0)) @ ht.T).abs().max()\n",
    "            Bq, B_amax_buffer = eden_1x16s_fp4_kernel_wrapper(\n",
    "                B,\n",
    "                ht,\n",
    "                1.0,\n",
    "                16,\n",
    "                current_amax=B_amax_buffer,\n",
    "            )\n",
    "            \n",
    "            accumulator += Aq @ Bq.T\n",
    "        accumulator /= acc_steps\n",
    "        \n",
    "        quad_err = (accumulator - A @ B.T).pow(2).mean() / (A @ B.T).pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        print(f\"{acc_steps}: {eff_bitwidth:.2f} bits\")\n",
    "        \n",
    "# NEED TO GROW BY ~1 bit per 4x samples\n",
    "# v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8013658",
   "metadata": {},
   "source": [
    "## Quartet_II linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmaxStorage:\n",
    "    def __init__(self):\n",
    "        self.e_ht_amax = None\n",
    "        self.weght_tht_amax = None\n",
    "        self.e_tht_amax = None\n",
    "        self.input_tht_amax = None\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        fields = [\n",
    "            (\"e_ht_amax\", self.e_ht_amax), \n",
    "            (\"weght_tht_amax\", self.weght_tht_amax), \n",
    "            (\"e_tht_amax\", self.e_tht_amax), \n",
    "            (\"input_tht_amax\", self.input_tht_amax)\n",
    "        ]\n",
    "        field_strs = []\n",
    "        for name, val in fields:\n",
    "            if val is not None:\n",
    "                try:\n",
    "                    v = val.item()\n",
    "                except Exception:\n",
    "                    v = val\n",
    "                field_strs.append(f\"{name}: {v:.3e}\")\n",
    "            else:\n",
    "                field_strs.append(f\"{name}: None\")\n",
    "        return \"<AmaxStorage \" + \", \".join(field_strs) + \">\"\n",
    "        \n",
    "\n",
    "class Quartet_II_fn(torch.autograd.Function):\n",
    "    group_size = 16\n",
    "    forward_scale_override = 1.0\n",
    "    backward_scale_override = (17 / 16) * 0.93\n",
    "    hadamard_matrix = get_hadamard_matrix(128, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    @torch.compile(dynamic=False)\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, amax_storage: AmaxStorage, delayed_amax: bool, disable_backward_quant: bool, four_over_six: bool):\n",
    "        ctx.batch = input.shape[0]\n",
    "        ctx.seq = input.shape[1]\n",
    "        ctx.in_dim = weight.shape[1]\n",
    "        ctx.out_dim = weight.shape[0]\n",
    "        ctx.delayed_amax = delayed_amax\n",
    "        ctx.amax_storage = amax_storage\n",
    "        ctx.disable_backward_quant = disable_backward_quant\n",
    "        \n",
    "        input_fp4 = rtn_1x16s_fp4_kernel_wrapper(input, scale_override=Quartet_II_fn.forward_scale_override, group_size=Quartet_II_fn.group_size, four_over_six=four_over_six)\n",
    "        weight_fp4 = rtn_1x16s_fp4_kernel_wrapper(weight.to(input.dtype), scale_override=Quartet_II_fn.forward_scale_override, group_size=Quartet_II_fn.group_size, four_over_six=four_over_six)\n",
    "\n",
    "        ctx.save_for_backward(input_fp4, weight_fp4)\n",
    "        return F.linear(input_fp4, weight_fp4)\n",
    "\n",
    "    @torch.compile(dynamic=False)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Load ctx and reshape\n",
    "        input_fp4, weight_fp4 = ctx.saved_tensors\n",
    "        \n",
    "        input_fp4 = input_fp4.reshape(ctx.batch * ctx.seq, ctx.in_dim)\n",
    "        grad_output = grad_output.reshape(ctx.batch * ctx.seq, ctx.out_dim)\n",
    "        \n",
    "        # Re-randomize the rotation\n",
    "        Quartet_II_fn.hadamard_matrix = Quartet_II_fn.hadamard_matrix.to(grad_output.dtype)\n",
    "        Quartet_II_fn.hadamard_matrix = rerotate_hadamard(Quartet_II_fn.hadamard_matrix)\n",
    "        \n",
    "        # No backward quant if flag\n",
    "        if ctx.disable_backward_quant:\n",
    "            grad_input = F.linear(\n",
    "                grad_output,\n",
    "                weight_fp4.T,\n",
    "                None,\n",
    "            ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "            \n",
    "            grad_weight = F.linear(\n",
    "                grad_output.T,\n",
    "                input_fp4.T,\n",
    "                None,\n",
    "            )\n",
    "            return grad_input, grad_weight, None, None, None, None\n",
    "        \n",
    "        # EW\n",
    "        if ctx.amax_storage.e_ht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.e_ht_amax = (grad_output.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max().float()\n",
    "        e_ht_fp4, ctx.amax_storage.e_ht_amax = eden_1x16s_fp4_kernel_wrapper(grad_output, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.e_ht_amax)\n",
    "        \n",
    "        if ctx.amax_storage.weght_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.weght_tht_amax = (weight_fp4.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max().float()\n",
    "        weight_tht_fp4, ctx.amax_storage.weght_tht_amax = eden_1x16s_fp4_kernel_wrapper(weight_fp4.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.weght_tht_amax)\n",
    "        \n",
    "        grad_input = F.linear(\n",
    "            e_ht_fp4,\n",
    "            weight_tht_fp4,\n",
    "            None,\n",
    "        ).view(ctx.batch, ctx.seq, ctx.in_dim)\n",
    "\n",
    "        # EtX\n",
    "        if ctx.amax_storage.e_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.e_tht_amax = (grad_output.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max().float()\n",
    "        e_tht_fp4, ctx.amax_storage.e_tht_amax = eden_1x16s_fp4_kernel_wrapper(grad_output.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.e_tht_amax)\n",
    "        \n",
    "        if ctx.amax_storage.input_tht_amax is None or not ctx.delayed_amax:\n",
    "            ctx.amax_storage.input_tht_amax = (input_fp4.T.reshape(-1, Quartet_II_fn.hadamard_matrix.size(0)) @ Quartet_II_fn.hadamard_matrix.T).abs().max().float()\n",
    "        input_tht_fp4, ctx.amax_storage.input_tht_amax = eden_1x16s_fp4_kernel_wrapper(input_fp4.T, Quartet_II_fn.hadamard_matrix, Quartet_II_fn.backward_scale_override, Quartet_II_fn.group_size, ctx.amax_storage.input_tht_amax)\n",
    "        \n",
    "        grad_weight = F.linear(\n",
    "            e_tht_fp4,\n",
    "            input_tht_fp4,\n",
    "            None,\n",
    "        )\n",
    "        \n",
    "        return grad_input, grad_weight, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb932bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quartet_II_linear(torch.nn.Linear):\n",
    "    def __init__(self, *args, delayed_amax=False, disable_backward_quant=False, four_over_six=True, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.delayed_amax = delayed_amax\n",
    "        self.disable_backward_quant = disable_backward_quant\n",
    "        self.four_over_six = four_over_six\n",
    "        self.amax_storage = AmaxStorage()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, disable_backward_quant=False):\n",
    "        return Quartet_II_fn.apply(x, self.weight, self.amax_storage, self.delayed_amax, self.disable_backward_quant, self.four_over_six)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7ab6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 4\n",
    "SEQ = 128\n",
    "HID = 256\n",
    "DELAYED_AMAX = True\n",
    "\n",
    "INPUT = torch.randn((BATCH, SEQ, HID), device='cuda')\n",
    "TARGET = torch.randn((BATCH, SEQ, 1), device='cuda')\n",
    "\n",
    "W1 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "W2 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "W3 = Quartet_II_linear(HID, HID, device='cuda', delayed_amax=DELAYED_AMAX)\n",
    "\n",
    "with torch.no_grad():\n",
    "    W1.weight /= (HID**0.5 * W1.weight.std())\n",
    "    W2.weight /= (HID**0.5 * W2.weight.std())\n",
    "    W3.weight /= (HID**0.5 * W3.weight.std())\n",
    "\n",
    "head = torch.randn(HID, 1, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a4616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/micromamba/envs/llmb/lib/python3.10/site-packages/torch/cuda/__init__.py:799: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "W1.weight.grad = None\n",
    "W2.weight.grad = None\n",
    "W3.weight.grad = None\n",
    "\n",
    "W1.disable_backward_quant = True\n",
    "W2.disable_backward_quant = True\n",
    "W3.disable_backward_quant = True\n",
    "\n",
    "hid = W1(INPUT)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W2(hid)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W3(hid)\n",
    "loss = (hid @ head - TARGET).pow(2).mean()\n",
    "loss.backward()\n",
    "\n",
    "w1_ref_grad = W1.weight.grad.clone().detach()\n",
    "w2_ref_grad = W2.weight.grad.clone().detach()\n",
    "w3_ref_grad = W3.weight.grad.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bac792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79a38b7a84447ec948c32117f646895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_steps=1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba39a36860a1428799ab515e4a108300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/micromamba/envs/llmb/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1391: UserWarning: Dynamo does not know how to trace the builtin `<unknown module>.Tensor.reshape.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\n",
      "If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\n",
      "If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n",
      "  torch._dynamo.utils.warn_once(explanation + \"\\n\" + \"\\n\".join(hints))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 2.21 bits, 0.996 cosine\n",
      "\tW2 grad err: 2.80 bits, 0.999 cosine\n",
      "\tW3 grad err: 4.06 bits, 1.002 cosine\n",
      "acc_steps=4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a94d69c0c7b44ae926e34173aeda05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 3.20 bits, 1.001 cosine\n",
      "\tW2 grad err: 3.81 bits, 1.003 cosine\n",
      "\tW3 grad err: 5.02 bits, 1.000 cosine\n",
      "acc_steps=16:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1312d82976d64ac799b75b753d021dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 4.20 bits, 0.998 cosine\n",
      "\tW2 grad err: 4.77 bits, 0.999 cosine\n",
      "\tW3 grad err: 6.09 bits, 0.999 cosine\n",
      "acc_steps=64:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465576d7ff844eaa9c31f2d8ae0f03e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 5.12 bits, 0.994 cosine\n",
      "\tW2 grad err: 5.76 bits, 0.998 cosine\n",
      "\tW3 grad err: 6.92 bits, 0.997 cosine\n",
      "acc_steps=256:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b744addf4a448d8dcfbd06401598c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 6.05 bits, 0.994 cosine\n",
      "\tW2 grad err: 6.67 bits, 0.997 cosine\n",
      "\tW3 grad err: 7.77 bits, 0.997 cosine\n",
      "acc_steps=1024:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae67b12bf29457eb8d6f8603cf94cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 6.79 bits, 0.994 cosine\n",
      "\tW2 grad err: 7.36 bits, 0.996 cosine\n",
      "\tW3 grad err: 8.63 bits, 0.999 cosine\n",
      "acc_steps=4096:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f064e81278b43ac997cea7f4f7dd4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW1 grad err: 7.22 bits, 0.994 cosine\n",
      "\tW2 grad err: 7.73 bits, 0.996 cosine\n",
      "\tW3 grad err: 8.44 bits, 0.997 cosine\n"
     ]
    }
   ],
   "source": [
    "W1.disable_backward_quant = False\n",
    "W2.disable_backward_quant = False\n",
    "W3.disable_backward_quant = False\n",
    "\n",
    "hid = W1(INPUT)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W2(hid)\n",
    "hid = torch.nn.functional.relu(hid)\n",
    "hid = W3(hid)\n",
    "loss = (hid @ head - TARGET).pow(2).mean()\n",
    "\n",
    "for acc_steps in tqdm([1, 4, 16, 64, 256, 1024, 4096]):\n",
    "    print(f\"{acc_steps=}:\")\n",
    "    W1.weight.grad = None\n",
    "    W2.weight.grad = None\n",
    "    W3.weight.grad = None\n",
    "    for _ in trange(acc_steps, leave=False):\n",
    "        loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        W1.weight.grad /= acc_steps\n",
    "        W2.weight.grad /= acc_steps\n",
    "        W3.weight.grad /= acc_steps\n",
    "    \n",
    "        quad_err = (W1.weight.grad - w1_ref_grad).pow(2).mean() / w1_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W1.weight.grad.flatten() @ w1_ref_grad.flatten()) / (w1_ref_grad.flatten() @ w1_ref_grad.flatten())\n",
    "        print(f\"\\tW1 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "        \n",
    "        quad_err = (W2.weight.grad - w2_ref_grad).pow(2).mean() / w2_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W2.weight.grad.flatten() @ w2_ref_grad.flatten()) / (w2_ref_grad.flatten() @ w2_ref_grad.flatten())\n",
    "        print(f\"\\tW2 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "        \n",
    "        quad_err = (W3.weight.grad - w3_ref_grad).pow(2).mean() / w3_ref_grad.pow(2).mean()\n",
    "        eff_bitwidth = (-torch.log2(quad_err) / 2).item()\n",
    "        cosine = (W3.weight.grad.flatten() @ w3_ref_grad.flatten()) / (w3_ref_grad.flatten() @ w3_ref_grad.flatten())\n",
    "        print(f\"\\tW3 grad err: {eff_bitwidth:.2f} bits, {cosine:.3f} cosine\")\n",
    "\n",
    "\n",
    "# NEED TO GROW BY ~1 bit per 4x samples\n",
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4e5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_shape(\n",
    "    in_dim, out_dim, batch_size, seq_len,\n",
    "    weight_dtype=torch.float32, act_dtype=torch.bfloat16, device='cuda',\n",
    "    warmup=10, rep=100,\n",
    "    compile=True, compile_kwargs=None,\n",
    "):\n",
    "    if compile_kwargs is None:\n",
    "        compile_kwargs = {'dynamic': False, 'mode': 'reduce-overhead', 'fullgraph': False}\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, in_dim, device=device, dtype=act_dtype)\n",
    "    \n",
    "    linear = Quartet_II_linear(in_dim, out_dim, four_over_six=True, device=device, dtype=weight_dtype)\n",
    "    if compile:\n",
    "        linear = torch.compile(linear, **compile_kwargs)\n",
    "    \n",
    "    # Forward\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: linear(x), warmup=warmup, rep=rep,\n",
    "    )\n",
    "    forward_time = ms\n",
    "\n",
    "    # Forward+Backward\n",
    "    grad = torch.randn_like(linear(x))\n",
    "    torch.set_grad_enabled(True)    \n",
    "    \n",
    "    def forward_backward(x, grad):\n",
    "        output = linear(x)\n",
    "        output.backward(grad)\n",
    "            \n",
    "    if compile:\n",
    "        compiled_forward_backward = torch.compile(forward_backward, **compile_kwargs)\n",
    "    \n",
    "    ms = triton.testing.do_bench(\n",
    "        lambda: compiled_forward_backward(x, grad), warmup=warmup, rep=rep,\n",
    "    )\n",
    "    total_time = ms\n",
    "\n",
    "    return {\n",
    "        \"forward_ms\": forward_time,\n",
    "        \"total_ms\": total_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41dfa2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2acd46e2124006976627c5d4eb9f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating model sizes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399050ae7ec24cee96770ce850fe5aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating shapes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 18:16:56.739000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [10/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:16:56.739000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [10/8]    function: 'torch_dynamo_resume_in_backward_at_81' (/tmp/ipykernel_135523/194258891.py:81)\n",
      "W0121 18:16:56.739000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [10/8]    last reason: 10/0: Cache line invalidated because L['___stack0'] got deallocated\n",
      "W0121 18:16:56.739000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [10/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:16:56.739000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [10/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "W0121 18:17:01.013000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [9/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:17:01.013000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [9/8]    function: 'torch_dynamo_resume_in_backward_at_62' (/tmp/ipykernel_135523/194258891.py:62)\n",
      "W0121 18:17:01.013000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [9/8]    last reason: 9/7: ctx.amax_storage.e_ht_amax is None                     \n",
      "W0121 18:17:01.013000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [9/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:17:01.013000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1aec8f7a1a4b52bbbd639477440cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating shapes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 18:17:06.342000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [16/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:17:06.342000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [16/8]    function: 'forward' (/tmp/ipykernel_135523/1317231249.py:10)\n",
      "W0121 18:17:06.342000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [16/8]    last reason: 16/7: GLOBAL_STATE changed: grad_mode \n",
      "W0121 18:17:06.342000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [16/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:17:06.342000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [16/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "W0121 18:17:06.345000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [1/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:17:06.345000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [1/8]    function: 'forward' (/tmp/ipykernel_135523/194258891.py:34)\n",
      "W0121 18:17:06.345000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [1/8]    last reason: 1/7: tensor 'input' size mismatch at index 2. expected 8192, actual 4096\n",
      "W0121 18:17:06.345000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [1/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:17:06.345000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "W0121 18:17:15.051000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [4/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:17:15.051000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [4/8]    function: 'backward' (/tmp/ipykernel_135523/194258891.py:51)\n",
      "W0121 18:17:15.051000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [4/8]    last reason: 4/7: ctx.in_dim == 4096                                     \n",
      "W0121 18:17:15.051000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:17:15.051000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c65666f33a04047a58e772d5bd7d61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating shapes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 18:17:27.162000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [18/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 18:17:27.162000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [18/8]    function: 'torch_dynamo_resume_in_forward_backward_at_29' (/tmp/ipykernel_135523/3247581210.py:29)\n",
      "W0121 18:17:27.162000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [18/8]    last reason: 18/7: tensor 'grad' size mismatch at index 2. expected 6144, actual 32768\n",
      "W0121 18:17:27.162000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [18/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 18:17:27.162000 135523 site-packages/torch/_dynamo/convert_frame.py:987] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "BATCH_SIZE=8\n",
    "SEQ_LEN = 2048\n",
    "\n",
    "shapes = {\n",
    "    # Q K V Down Up Gate Down\n",
    "    # \"100M\": [(1024 * 3, 1024), (1024, 1024), (2816 * 2, 1024), (1024, 2816)],\n",
    "    # \"800M\": [(2048 * 3, 2048), (2048, 2048), (5632 * 2, 2048), (2048, 5632)],\n",
    "    \"3B\": [(3072 * 3, 3072), (3072, 3072), (8192 * 2, 3072), (3072, 8192)],\n",
    "    \"7B\": [(4096 * 3, 4096), (4096, 4096), (11008 * 2, 4096), (4096, 11008)],\n",
    "    \"22B\": [(6144 * 3, 6144), (6144, 6144), (16384 * 2, 6144), (6144, 16384)],\n",
    "    # \"52B\": [(8192 * 3, 8192), (8192, 8192), (22016 * 2, 8192), (8192, 22016)],\n",
    "}\n",
    "\n",
    "shape_to_result = {}\n",
    "\n",
    "for size, model_shapes in tqdm(shapes.items(), desc=\"Iterating model sizes\"):\n",
    "    for shape in tqdm(model_shapes, desc=\"Iterating shapes\", leave=False):\n",
    "        if shape not in shape_to_result:\n",
    "            result = bench_shape(\n",
    "                shape[1], shape[0], BATCH_SIZE, SEQ_LEN,\n",
    "            )\n",
    "            shape_to_result[shape] = result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b405a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3B: 20.671732938976817ms forward, 107.73571999867757ms foward+backward\n",
      "7B: 36.1965014255408ms forward, 170.70031929016113ms foward+backward\n",
      "22B: 79.49745286794808ms forward, 650.2594998677571ms foward+backward\n"
     ]
    }
   ],
   "source": [
    "for size, model_shapes in shapes.items():\n",
    "    forward_latency = sum(shape_to_result[shape]['forward_ms'] for shape in model_shapes)\n",
    "    total_latency = sum(shape_to_result[shape]['total_ms'] for shape in model_shapes)\n",
    "    \n",
    "    print(f\"{size}: {forward_latency}ms forward, {total_latency}ms foward+backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e4204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd4591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
